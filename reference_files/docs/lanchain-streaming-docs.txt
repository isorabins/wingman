LangChain Streaming Documentation
Overview
Streaming in LangChain significantly improves application responsiveness by displaying LLM-generated outputs progressively, enhancing user experience (UX), especially important given typical LLM latency.
What to Stream
1. Streaming LLM Outputs
	•	Real-time LLM outputs to provide immediate feedback and reduce perceived wait time.
2. Streaming Pipeline or Workflow Progress
	•	LangGraph Workflows: Track real-time updates to the graph state, monitoring active nodes.
	•	LCEL Pipelines: Stream updates from sub-runnables, providing insights into overall pipeline execution.
3. Streaming Custom Data
	•	Stream additional custom information directly from workflow steps (tools or LangGraph nodes) for detailed execution insights.
Streaming APIs
LangChain supports two primary APIs for streaming, applicable to any Runnable (e.g., LLMs, LangGraph graphs, LCEL pipelines):
	1	stream() (sync) and astream() (async):
	◦	Stream outputs from Runnables incrementally.
	2	astream_events() (async only):
	◦	Provides access to custom events and intermediate outputs for LCEL-based applications.
	◦	Not typically needed for LangGraph workflows.
Legacy:
	•	astream_log() (deprecated, not recommended for new projects).
Using stream() and astream()
	•	These methods yield chunks of output as generated by components.
	•	Example usage: for chunk in component.stream(input):
	•	    print(chunk)  # Process efficiently to avoid upstream delays
	•	
Usage with Chat Models
	•	Streams outputs as AIMessageChunks, useful for interactive applications.
Usage with LangGraph
	•	Compiled graphs support standard streaming APIs.
	•	Select from streaming modes:
	◦	values: Emit all state values.
	◦	updates: Emit node names and updates.
	◦	debug: Emit debug information.
	◦	messages: Emit LLM messages token-by-token.
	◦	custom: Emit custom data via LangGraph's StreamWriter.
Usage with LCEL
	•	By default, streams output from the final step.
	•	To access intermediate results, use astream_events() or structured callbacks.
Example (astream_events):
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-3-sonnet-20240229")
prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for event in chain.astream_events({"topic": "parrot"}):
    if event["event"] == "on_chat_model_stream":
        print(event, end="|", flush=True)
Writing Custom Data to the Stream
	•	LangGraph: Use StreamWriter to emit custom data through stream/astream.
	•	LCEL: Use dispatch_events/adispatch_events to surface custom data through astream_events().
"Auto-Streaming" Chat Models
	•	LangChain automatically streams chat model outputs even when using non-streaming invoke methods if streaming is detected elsewhere in the app.
Example:
def node(state):
    ai_message = model.invoke(state["messages"])  # Auto-switches to streaming

for chunk in compiled_graph.stream(..., mode="messages"):
    ...
Async Programming
	•	Use asynchronous methods (e.g., ainvoke, astream) consistently for non-blocking workflows.
	•	Verify correct async method usage if streaming doesn't appear in real-time.
Related Resources
	•	LangGraph streaming conceptual guide
	•	LangGraph streaming how-to guides
	•	How to stream runnables
	•	How to stream chat models
	•	How to stream tool calls
	•	Streaming custom data (LangGraph)
	•	Dispatching custom callback events (LCEL)
