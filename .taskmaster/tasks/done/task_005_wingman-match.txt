# Task ID: 5
# Title: Confidence assessment agent and flow
# Status: pending
# Dependencies: 2, 4
# Priority: high
# Description: Fork creativity assessment to ConfidenceTestAgent with 12 dating questions and 6 archetypes.
# Details:
- Create new files based on reference patterns (do NOT modify reference files):
  - src/agents/confidence_agent.py (ported from creativity_agent.py)
  - src/assessment/confidence_scoring.py (pure scoring functions: score_answers, choose_archetype, compute_level)
  - src/api/assessment_routes.py or extend src/main.py with assessment endpoints
- Implement 12-question multi-choice flow with 6 archetypes (Analyzer, Sprinter, Ghost, Scholar, Naturalist, Protector)
- Compute primary archetype + experience level (beginner/intermediate/advanced) from scoring thresholds
- Persist to confidence_test_results with timestamps and JSON answers
- Add summary generation via content_summarizer.py for profile hints
- Follow TDD approach: implement pure functions first, then write unit tests with fixed answer vectors
<info added on 2025-08-11T03:42:15.289Z>
## Context7 FastAPI + Pydantic v2 Best Practices for Assessment Endpoints

- Define strict request/response models using Pydantic:
  ```python
  class ConfidenceAssessmentRequest(BaseModel):
      user_id: UUID
      answers: list[int] = Field(..., min_length=12, max_length=12)
      
  class ConfidenceResult(BaseModel):
      archetype: str
      level: str
      scores: dict[str, float]
      summary: str | None = None
  ```

- Use `response_model=` parameter in endpoint definitions; avoid returning naked dictionaries
- Leverage `.model_validate()` for request parsing and `.model_dump()` for responses
- Use `pydantic_settings.BaseSettings` for environment configuration
- Add field constraints with `Field(..., ge=0, le=5, max_length=50)` for validation
- Implement validation-first approach: pure scoring functions handle all business logic, FastAPI endpoints only orchestrate
- Use `BackgroundTasks` for non-critical side effects (summary generation, analytics)
- Prefer explicit unions (`str | None`) over `Any` types
- Avoid `response_model=None` unless streaming or returning binary content
- Document HTTP status codes and provide examples in OpenAPI via `responses={...}` parameter:
  ```python
  @router.post("/api/assessment/confidence", 
               response_model=ConfidenceResult,
               responses={
                   200: {"description": "Assessment processed successfully"},
                   422: {"description": "Invalid assessment data"}
               })
  ```
</info added on 2025-08-11T03:42:15.289Z>

# Test Strategy:
- TDD approach: implement pure functions first, then write unit tests with fixed answer vectors
- Deterministic unit tests for scoring in tests/backend/test_confidence_scoring.py
- Property tests for boundary thresholds
- Integration tests in tests/backend/test_confidence_endpoints.py: POST then GET returns same; resubmission overwrites

# Subtasks:
## 1. Design question bank and 6-archetype scoring rubric [pending]
### Dependencies: None
### Description: Author 12 multiple-choice dating-context questions and map each answer option to weighted scores across the six archetypes: Analyzer, Sprinter, Ghost, Scholar, Naturalist, Protector.
### Details:
Define clear behavioral signals per archetype; ensure balanced coverage across questions; document scoring weights; prepare localization-ready structures; version the bank for future updates.

## 2. Define experience level thresholds and rules [pending]
### Dependencies: None
### Description: Establish deterministic thresholds to classify experience level (beginner/intermediate/advanced) from aggregate scores, including tie-breaking and boundary semantics.
### Details:
Specify numeric ranges, inclusive/exclusive edges, and handling for low-response or neutral patterns; document tie resolution between archetypes and levels.

## 3. Implement pure scoring functions in confidence_scoring.py [pending]
### Dependencies: 5.1, 5.2
### Description: Create deterministic, side-effect-free functions for scoring answers, selecting primary archetype, and computing experience levels in src/assessment/confidence_scoring.py.
### Details:
Implement three core functions with docstrings: score_answers, choose_archetype, compute_level. Functions should accept answer vectors and scoring config; return archetype scores, primary archetype, level, and metadata; ensure stable sort/tie-breaks. Follow TDD approach by implementing these pure functions first.

## 4. Create ConfidenceTestAgent in confidence_agent.py [pending]
### Dependencies: 5.3
### Description: Create src/agents/confidence_agent.py based on reference_files/src/agents/creativity_agent.py structure and flow.
### Details:
Port the agent class while maintaining the 12-question multi-choice flow; wire the agent to call pure functions from confidence_scoring.py; do NOT modify reference files.

## 5. Persistence layer for confidence_test_results [pending]
### Dependencies: 5.3, 5.4
### Description: Implement save/load to confidence_test_results with timestamps and JSON answers, ensuring backward compatibility and schema validation.
### Details:
Define schema fields (user_id UUID, created_at TIMESTAMP, answers_json JSONB, archetype TEXT, level TEXT, scores_json JSONB, version INT); add repository/service methods with retries and error handling.

## 6. Implement assessment endpoints [pending]
### Dependencies: 5.4, 5.5
### Description: Create assessment endpoints in src/api/assessment_routes.py or extend src/main.py following FastAPI style from reference.
### Details:
Implement POST /api/assessment/confidence for submitting assessment and GET /api/assessment/results for retrieving results. Follow FastAPI patterns from reference_files/src/main.py. POST should return {archetype, level, scores}; GET should return the latest result for the user_id.

## 7. Summary generation via content_summarizer [pending]
### Dependencies: 5.3, 5.4
### Description: Add profile hint generation that converts results into concise, user-facing summaries using content_summarizer.py.
### Details:
Follow summary patterns from reference_files/src/content_summarizer.py; craft prompt/templates using archetype and level; ensure deterministic stubs for tests; include safety guards and length limits.

## 8. Write unit tests for confidence scoring [pending]
### Dependencies: 5.3
### Description: Create comprehensive unit tests in tests/backend/test_confidence_scoring.py for the pure scoring functions.
### Details:
Create fixtures of answer vectors; implement property tests for boundary edges; test tie-breaking logic and edge cases; ensure deterministic scoring.

## 9. Write integration tests for confidence endpoints [pending]
### Dependencies: 5.6
### Description: Implement integration tests in tests/backend/test_confidence_endpoints.py for the assessment endpoints.
### Details:
Test POST /api/assessment/confidence followed by GET /api/assessment/results; verify data consistency; test resubmission overwrites previous results.

